{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X5WgjPL_1Hij"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#from utils import split_last, merge_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sn2PCMjE4gPS"
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"A layernorm module in the TF style (epsilon inside the square root).\"\n",
    "    def __init__(self, cfg, variance_epsilon=1e-12):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(cfg.dim))\n",
    "        self.beta  = nn.Parameter(torch.zeros(cfg.dim))\n",
    "        self.variance_epsilon = variance_epsilon\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
    "        return self.gamma * x + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WzX4ddh04R7X"
   },
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    \"The embedding module from word, position and token_type embeddings.\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_embed = nn.Embedding(cfg.vocab_size, cfg.dim)  # token embedding\n",
    "        self.pos_embed = nn.Embedding(cfg.max_len, cfg.dim)     # position embedding\n",
    "        self.seg_embed = nn.Embedding(cfg.n_segments, cfg.dim)  # segment(token type) embedding\n",
    "\n",
    "        self.norm = LayerNorm(cfg)\n",
    "        self.drop = nn.Dropout(cfg.p_drop_hidden)\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(seq_len, dtype=torch.long, device=x.device)\n",
    "        pos = pos.unsqueeze(0).expand_as(x) # (S,) -> (B, S)\n",
    "\n",
    "        e = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
    "        return self.drop(self.norm(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YhIe-ald4TnC"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer( nn.Module ) :\n",
    "    def __init__( self, config ) :\n",
    "        super().__init__()\n",
    "        self.valueLinearProjection = nn.Linear( config.hidden_dim, config.hidden_dim )\n",
    "        self.queryLinearProjection = nn.Linear( config.hidden_dim, config.hidden_dim )\n",
    "        self.keyLinearProjection = nn.Linear( config.hidden_dim, config.hidden_dim )\n",
    "        self.dropout = nn.Dropout( config.drop_attn )\n",
    "        self.n_heads = config.n_heads\n",
    "\n",
    "    def forward( self, x, mask ) :\n",
    "        \"\"\"\n",
    "        x, query(q), key(k), value(v) : B(batch_size), S(seq_len), D(dim)\n",
    "        mask : B X S\n",
    "        \"\"\"\n",
    "        # (B, S, H, W)\n",
    "        q, k, v = self.valueLinearProjection( x ), self.queryLinearProjection( x ), self.keyLinearProjection( x )\n",
    "        # (B, H, S, W)\n",
    "        q, k, v = ( x.view( *x.size()[:-1], n_heads, x.shape(-1) / n_head ).transpose( 1, 2 )\n",
    "                    for x in [q, k, v] )\n",
    "        #q, k, v = ( split_last( x, ( self.n_heads, -1 ) ).transpose( 1, 2 )\n",
    "        #            for x in [q, k, v] )\n",
    "        # (B, H, S, W) @ (B, H, W, S) -> (B, H, S, S)\n",
    "        scores = q @ k.transpose(-2, -1) / np.sqrt( k.size(-1) )\n",
    "        if mask is not None :\n",
    "            mask = mask[:, None, None, :].float()\n",
    "            scores -= 10000.0 * (1.0 - mask) \n",
    "            # Make Masked Area in a very small value\n",
    "            # So we can ignore the masked Value\n",
    "        scores = self.dropout( torch.F.softmax( scores, dim=-1 ) )\n",
    "        # (B, H, S, S) @ (B, H, S, W) -> (B, H, S, W) -transpose-> (B, S, H, W)\n",
    "        h = ( scores @ v ).transpose( 1, 2 ).contiguous()\n",
    "        h = h.view( *h.size()[:-2], -1 ) # (B, S, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "spaR3DSdNlwB"
   },
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward( nn.Module ) :\n",
    "    def __init__( self, config ) :\n",
    "        self.relu = nn.ReLU()\n",
    "        self.forward1 = nn.Linear( config.dim, config.dimff )\n",
    "        self.forward2 = nn.Linear( config.dimff, config.dim )\n",
    "\n",
    "    def forward( self, x ) :\n",
    "        return self.forward2( self.relu( self.forward1( x ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OrEMS11LHPMD"
   },
   "outputs": [],
   "source": [
    "class Block( nn.Module ) :\n",
    "    def __init__( slef, config ) :\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttentionLayer( config )\n",
    "        self.proj = nn.Linear( config.dim, config.dim )\n",
    "        self.norm1 = LayerNorm( config )\n",
    "        self.pwff = PositionWiseFeedForward( config )\n",
    "        self.norm2 = LayerNorm( config )\n",
    "        self.drop = nn.Dropout( config.dropout )\n",
    "\n",
    "    def forward( self, x, mask ) :\n",
    "        a = self.attn( x, mask )\n",
    "        h = self.norm1( x + self.drop( self.proj( a ) ) )\n",
    "        h = self.norm2( h + self.drop( self.pwff( h ) ) )\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qKnyKllNHQld"
   },
   "outputs": [],
   "source": [
    "class Transformer( nn.Module ) :\n",
    "    def __init__( self, config ) :\n",
    "        self.embedding = Embeddings( config )\n",
    "        self.blocks = [ Block( config ) for _ in range( config.num_layers ) ]\n",
    "\n",
    "    def forward( self, x, seg, mask ) :\n",
    "        embedding = self.embedding( x, seg )\n",
    "        for block in self.blocks :\n",
    "            embedding = block( embedding, mask )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J1c1Ji_ECxIq"
   },
   "outputs": [],
   "source": [
    "# 1. Input x = [x1, x2, ..., xn]\n",
    "# 2. MLM select a random set of positions to mask out m = [m1, ..., mk]^3\n",
    "# 3. the token in the selected positions are replaced with a [MASK] token -> xm = replace( x, m, [MASK] )\n",
    "# 4. generator learns to predict the original identities of the masked-out tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wrsqF0f7M9_J"
   },
   "outputs": [],
   "source": [
    "from random import randint, shuffle\n",
    "from random import random as rand\n",
    "\n",
    "class PreprocessGenerator( Pipeline ):\n",
    "    \n",
    "    def __init__( self, vacab_words, indexer ) :\n",
    "        super().__init__()\n",
    "        self.max_pred = 20        # max tokens of prediction\n",
    "        self.mask_prob = 0.15     # mask coverage (following mlm)\n",
    "        self.vocab_words = vocab_words\n",
    "        self.indexer = indexer    # function from token to token index\n",
    "        self.max_len = 512\n",
    "\n",
    "    def __call__( self, instance ) :\n",
    "        is_next, tokens_a, tokens_b = instance\n",
    "\n",
    "        # special tokens [CLS], [SEP], [SEP]\n",
    "        truncate_tokens_pair( tokens_a, tokens_b, self.max_len - 3 )\n",
    "\n",
    "        # Add special tokens\n",
    "        tokens = ['[CLS]'] + tokens_a + ['[SEP]'] + tokens_b + ['[SEP]']\n",
    "        segment_idx = [0] * ( len(tokens_a) + 2 ) \n",
    "                    + [1] * ( len(tokens_b) + 1 )\n",
    "        input_mask = [1] * len( tokens )\n",
    "\n",
    "        # For masked language model (MLM)\n",
    "        masked_tokens, maksed_pos = [], []\n",
    "        n_pred = min( self.max_pred, max( 1, int( round( len( tokens ) * self.mask_prob ) ) ) )\n",
    "        candidate_pos = [ i for i, token in enumerate( tokens ) \n",
    "                            if token != '[CLS]' and token != '[SEP]' ]\n",
    "        shuffle( candidate_pos )\n",
    "\n",
    "        for pos in cadidate_pos[ :n_pred ] :\n",
    "            masked_tokens.append( tokens[ pos ] )\n",
    "            masked_pos.append( pos )\n",
    "            if rand() < 0.8 : #80%\n",
    "                tokens[ pos ] = '[MASK]'\n",
    "            elif rand() < 0.5 : #10%\n",
    "                tokens[ pos ] = get_random_word( self.vocab_words )\n",
    "        masked_weights = [1] * len( masked_tokens )\n",
    "\n",
    "        # Token Indexing\n",
    "        input_idx = self.indexer( tokens )\n",
    "        maksed_idx = self.indexer( masked_tokens )\n",
    "        \n",
    "        # Zero Padding\n",
    "        n_pad = self.max_len - len( input_idx )\n",
    "        input_idx.extend( [0] * n_pad )\n",
    "        segment_idx.extend( [0] * n_pad )\n",
    "        input_mask.extend( [0] * n_pad )\n",
    "\n",
    "        # Zero Padding for Masked Target\n",
    "        if self.max_pred > n_pred :\n",
    "            n_pad = self.max_pred - n_pred\n",
    "            masked_idx.extend( [0] * n_pad )\n",
    "            masked_pos.extend( [0] * n_pad )\n",
    "            masked_weights.extend( [0] * n_pad )\n",
    "        \n",
    "        return ( input_idx, segment_idx, input_mask, masked_idx, masked_pos, masked_weights, is_next )\n",
    "\n",
    "    def truncate_tokens_pair( tokens_a, tokens_b, max_len ):\n",
    "        while True :\n",
    "            if len( tokens_a ) + len( tokens_b ) <= max_len :\n",
    "                break\n",
    "            if len( tokens_a ) > len( tokens_b ) :\n",
    "                tokens_a.pop()\n",
    "            else :\n",
    "                tokens_b.pop()\n",
    "\n",
    "    def get_random_word( vocab_words ) :\n",
    "        i = randint( 0, len( vocab_words ) - 1 )\n",
    "        return vocab_words[ i ]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "m_Zih3E3Fqop"
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-4-72d48d2235d4>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-72d48d2235d4>\"\u001b[0;36m, line \u001b[0;32m15\u001b[0m\n\u001b[0;31m    .embedding\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "class Generator( nn.Module ) :\n",
    "\n",
    "    def __init__( self, config ) :\n",
    "        super().__init__()\n",
    "        self.transformer    = Transformer( config )\n",
    "        self.fc             = nn.Linear( config.dim, config.dim )\n",
    "        self.activ1         = nn.Tanh()\n",
    "        self.linear         = nn.Linear( config.dim, config.dim )\n",
    "        self.activ2         = nn.ReLU()\n",
    "        self.norm           = LayerNorm( config )\n",
    "        self.classifier     = nn.Linear( config.dim, 2 )\n",
    "\n",
    "        # Decoder\n",
    "        embed_weight        = self.transformer\n",
    "                                  .embedding\n",
    "                                  .tok_embed\n",
    "                                  .weight\n",
    "        n_vocab, n_dim      = embed_weight.size()\n",
    "        self.decoder        = nn.Linear( n_dim, n_vocab, bias=False )\n",
    "        self.decoder.weight = embed_weight\n",
    "        self.decoder_bias   = nn.Parameter( torch.zeros( n_vocab) )\n",
    "\n",
    "    def forward( self, input_idx, segment_idx, input_mask, masked_pos ) :\n",
    "        h = self.transformer( input_idx, segment_id, input_mask, masked_pos )\n",
    "        pooled_h = self.activ1( self.fc( h[:,0] ) )\n",
    "        masked_pos = masked_pos[:, :, None].expand( -1, -1, h.size( -1 ) )\n",
    "        h_masked = torch.gather( h, 1, masked_pos )\n",
    "        h_masked = self.norm( self.active2( self.linear( h_masked ) ) )\n",
    "        logits_lm = self.decoder( h_masked ) + self.decoder_bias\n",
    "        logits_clsf = self.classifier( pooled_h )\n",
    "\n",
    "        return logits_lm, logits_clsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module): \n",
    "    def __init__(self, Config):\n",
    "        super().__init__() \n",
    "        self.transformer = Transformers(Config) # 기본 코드에서 트랜스포머를 가져온다. -> 이제 여기에 Generator Head라고 되어있는 애의 내용물을 붙여주면 된다.\n",
    "        self.dense = nn.Linear(Config.dim, Config.dim)  # 가중치를 N개로 고려를 다 해서 전처리를 해준다? 이전에는 하나의 노드가 K개만 소통하고 있기 때문에, dense를 하기 전과 후의 가중치가 달라지므로.\n",
    "        # Linear 자체가 Weight를 다시 계산을 해주는 거니까.\n",
    "        # self.activation = F.gelu\n",
    "        # self.norm = nn.LayerNorm(dim, eps=layer_norm_eps)\n",
    "        self.classifier = nn.Linear(Config.dim, 1) # Original Replace / 2차원으로 하면 되지 않나? \n",
    "    \n",
    "    def forward(self, x, seg,is_replaced_label = None, mask=None):\n",
    "      hidden_states = self.transformer(x, seg, mask)  # Batch * Sequence Length * Dim\n",
    "      \n",
    "      h = self.dense(hidden_states) # 처음에 댄스에 주어진 히든 스테이트가 주어진다. 이 히든 스테이트는 아마 self.transformer를 거쳐서 나온 애일 것. 히든 스테이트 \n",
    "      # -> Batch * Sequence Length * Config.Dim\n",
    "\n",
    "      h = self.activation(h)\n",
    "      \n",
    "      h = self.norm(h)\n",
    "      logits = self.classifier(hidden_states)\n",
    "      # Classifier ->Batch * Sequence Length * 1  == logits\n",
    "      outputs = (logits,) \n",
    "      \n",
    "      if is_replaced_label is not None: \n",
    "        loss_fct = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        if input_mask is not None: \n",
    "          # active_loss.shape = (-1, Sequence Length) -> 1인 애들만 찾아서 True False 행렬로 바꿔준다.\n",
    "          active_loss = mask.view(-1, hidden_states.shape[1])==1\n",
    "\n",
    "          # active_logits = Batch * Sequence Length * 1 -> (-1, Sequence Length) * [true false와 곱해준다.]\n",
    "          active_logits = logits.view(-1, hidden_states.shape[1])[active_loss] # false인 친구들은 이제 사라짐 ㅠ\n",
    "          active_labels = is_replaced_label[active_loss] # 그리고 이제 여기서 True False인 애들을 남겨서 -> 주어진 정답인 is_replaced_label과 곱해주면 active_labesl가 나오고\n",
    "          disc_loss = loss_fct(active_logits, active_labels.float()) # 그리고 이제 여기서 loss_function을 먹여서 disc_loss를 계산해주고\n",
    "        else:\n",
    "          disc_loss = loss_fct(logits.view(-1, hidden_states.shape[1]), is_replaced_label.float())\n",
    "\n",
    "        outputs += (disc_loss, ) # 아웃풋에 더해주고\n",
    "\n",
    "      return outputs # 보내줍니다.  \n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "210709_BERT_Implementation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
