{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "210709_BERT_Implementation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5WgjPL_1Hij"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "#from utils import split_last, merge_last"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sn2PCMjE4gPS"
      },
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"A layernorm module in the TF style (epsilon inside the square root).\"\n",
        "    def __init__(self, cfg, variance_epsilon=1e-12):\n",
        "        super().__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(cfg.dim))\n",
        "        self.beta  = nn.Parameter(torch.zeros(cfg.dim))\n",
        "        self.variance_epsilon = variance_epsilon\n",
        "\n",
        "    def forward(self, x):\n",
        "        u = x.mean(-1, keepdim=True)\n",
        "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
        "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
        "        return self.gamma * x + self.beta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzX4ddh04R7X"
      },
      "source": [
        "class Embeddings(nn.Module):\n",
        "    \"The embedding module from word, position and token_type embeddings.\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_embed = nn.Embedding(cfg.vocab_size, cfg.dim)  # token embedding\n",
        "        self.pos_embed = nn.Embedding(cfg.max_len, cfg.dim)     # position embedding\n",
        "        self.seg_embed = nn.Embedding(cfg.n_segments, cfg.dim)  # segment(token type) embedding\n",
        "\n",
        "        self.norm = LayerNorm(cfg)\n",
        "        self.drop = nn.Dropout(cfg.p_drop_hidden)\n",
        "\n",
        "    def forward(self, x, seg):\n",
        "        seq_len = x.size(1)\n",
        "        pos = torch.arange(seq_len, dtype=torch.long, device=x.device)\n",
        "        pos = pos.unsqueeze(0).expand_as(x) # (S,) -> (B, S)\n",
        "\n",
        "        e = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
        "        return self.drop(self.norm(e))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhIe-ald4TnC"
      },
      "source": [
        "class MultiHeadAttentionLayer( nn.Module ) :\n",
        "    def __init__( self, config ) :\n",
        "        super().__init__()\n",
        "        self.valueLinearProjection = nn.Linear( config.hidden_dim, config.hidden_dim )\n",
        "        self.queryLinearProjection = nn.Linear( config.hidden_dim, config.hidden_dim )\n",
        "        self.keyLinearProjection = nn.Linear( config.hidden_dim, config.hidden_dim )\n",
        "        self.dropout = nn.Dropout( config.drop_attn )\n",
        "        self.n_heads = config.n_heads\n",
        "\n",
        "    def forward( self, x, mask ) :\n",
        "        \"\"\"\n",
        "        x, query(q), key(k), value(v) : B(batch_size), S(seq_len), D(dim)\n",
        "        mask : B X S\n",
        "        \"\"\"\n",
        "        # (B, S, H, W)\n",
        "        q, k, v = self.valueLinearProjection( x ), self.queryLinearProjection( x ), self.keyLinearProjection( x )\n",
        "        # (B, H, S, W)\n",
        "        q, k, v = ( x.view( *x.size()[:-1], n_heads, x.shape(-1) / n_head ).transpose( 1, 2 )\n",
        "                    for x in [q, k, v] )\n",
        "        #q, k, v = ( split_last( x, ( self.n_heads, -1 ) ).transpose( 1, 2 )\n",
        "        #            for x in [q, k, v] )\n",
        "        # (B, H, S, W) @ (B, H, W, S) -> (B, H, S, S)\n",
        "        scores = q @ k.transpose(-2, -1) / np.sqrt( k.size(-1) )\n",
        "        if mask is not None :\n",
        "            mask = mask[:, None, None, :].float()\n",
        "            scores -= 10000.0 * (1.0 - mask) \n",
        "            # Make Masked Area in a very small value\n",
        "            # So we can ignore the masked Value\n",
        "        scores = self.dropout( torch.F.softmax( scores, dim=-1 ) )\n",
        "        # (B, H, S, S) @ (B, H, S, W) -> (B, H, S, W) -transpose-> (B, S, H, W)\n",
        "        h = ( scores @ v ).transpose( 1, 2 ).contiguous()\n",
        "        h = h.view( *h.size()[:-2], -1 ) # (B, S, D)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spaR3DSdNlwB"
      },
      "source": [
        "class PositionWiseFeedForward( nn.Module ) :\n",
        "    def __init__( self, config ) :\n",
        "        self.relu = nn.ReLU()\n",
        "        self.forward1 = nn.Linear( config.dim, config.dimff )\n",
        "        self.forward2 = nn.Linear( config.dimff, config.dim )\n",
        "\n",
        "    def forward( self, x ) :\n",
        "        return self.forward2( self.relu( self.forward1( x ) ) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrEMS11LHPMD"
      },
      "source": [
        "class Block( nn.Module ) :\n",
        "    def __init__( slef, config ) :\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttentionLayer( config )\n",
        "        self.proj = nn.Linear( config.dim, config.dim )\n",
        "        self.norm1 = LayerNorm( config )\n",
        "        self.pwff = PositionWiseFeedForward( config )\n",
        "        self.norm2 = LayerNorm( config )\n",
        "        self.drop = nn.Dropout( config.dropout )\n",
        "\n",
        "    def forward( self, x, mask ) :\n",
        "        a = self.attn( x, mask )\n",
        "        h = self.norm1( x + self.drop( self.proj( a ) ) )\n",
        "        h = self.norm2( h + self.drop( self.pwff( h ) ) )\n",
        "        return h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKnyKllNHQld"
      },
      "source": [
        "class Transformer( nn.Module ) :\n",
        "    def __init__( self, config ) :\n",
        "        self.embedding = Embeddings( config )\n",
        "        self.blocks = [ Block( config ) for _ in range( config.num_layers ) ]\n",
        "\n",
        "    def forward( self, x, seg, mask ) :\n",
        "        embedding = self.embedding( x, seg )\n",
        "        for block in self.blocks :\n",
        "            embedding = block( embedding, mask )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1c1Ji_ECxIq"
      },
      "source": [
        "# 1. Input x = [x1, x2, ..., xn]\n",
        "# 2. MLM select a random set of positions to mask out m = [m1, ..., mk]^3\n",
        "# 3. the token in the selected positions are replaced with a [MASK] token -> xm = replace( x, m, [MASK] )\n",
        "# 4. generator learns to predict the original identities of the masked-out tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrsqF0f7M9_J"
      },
      "source": [
        "from random import randint, shuffle\n",
        "from random import random as rand\n",
        "\n",
        "class PreprocessGenerator( Pipeline ):\n",
        "    \n",
        "    def __init__( self, vacab_words, indexer ) :\n",
        "        super().__init__()\n",
        "        self.max_pred = 20        # max tokens of prediction\n",
        "        self.mask_prob = 0.15     # mask coverage (following mlm)\n",
        "        self.vocab_words = vocab_words\n",
        "        self.indexer = indexer    # function from token to token index\n",
        "        self.max_len = 512\n",
        "\n",
        "    def __call__( self, instance ) :\n",
        "        is_next, tokens_a, tokens_b = instance\n",
        "\n",
        "        # special tokens [CLS], [SEP], [SEP]\n",
        "        truncate_tokens_pair( tokens_a, tokens_b, self.max_len - 3 )\n",
        "\n",
        "        # Add special tokens\n",
        "        tokens = ['[CLS]'] + tokens_a + ['[SEP]'] + tokens_b + ['[SEP]']\n",
        "        segment_idx = [0] * ( len(tokens_a) + 2 ) \n",
        "                    + [1] * ( len(tokens_b) + 1 )\n",
        "        input_mask = [1] * len( tokens )\n",
        "\n",
        "        # For masked language model (MLM)\n",
        "        masked_tokens, maksed_pos = [], []\n",
        "        n_pred = min( self.max_pred, max( 1, int( round( len( tokens ) * self.mask_prob ) ) ) )\n",
        "        candidate_pos = [ i for i, token in enumerate( tokens ) \n",
        "                            if token != '[CLS]' and token != '[SEP]' ]\n",
        "        shuffle( candidate_pos )\n",
        "\n",
        "        for pos in cadidate_pos[ :n_pred ] :\n",
        "            masked_tokens.append( tokens[ pos ] )\n",
        "            masked_pos.append( pos )\n",
        "            if rand() < 0.8 : #80%\n",
        "                tokens[ pos ] = '[MASK]'\n",
        "            elif rand() < 0.5 : #10%\n",
        "                tokens[ pos ] = get_random_word( self.vocab_words )\n",
        "        masked_weights = [1] * len( masked_tokens )\n",
        "\n",
        "        # Token Indexing\n",
        "        input_idx = self.indexer( tokens )\n",
        "        maksed_idx = self.indexer( masked_tokens )\n",
        "        \n",
        "        # Zero Padding\n",
        "        n_pad = self.max_len - len( input_idx )\n",
        "        input_idx.extend( [0] * n_pad )\n",
        "        segment_idx.extend( [0] * n_pad )\n",
        "        input_mask.extend( [0] * n_pad )\n",
        "\n",
        "        # Zero Padding for Masked Target\n",
        "        if self.max_pred > n_pred :\n",
        "            n_pad = self.max_pred - n_pred\n",
        "            masked_idx.extend( [0] * n_pad )\n",
        "            masked_pos.extend( [0] * n_pad )\n",
        "            masked_weights.extend( [0] * n_pad )\n",
        "        \n",
        "        return ( input_idx, segment_idx, input_mask, masked_idx, masked_pos, masked_weights, is_next )\n",
        "\n",
        "    def truncate_tokens_pair( tokens_a, tokens_b, max_len ):\n",
        "        while True :\n",
        "            if len( tokens_a ) + len( tokens_b ) <= max_len :\n",
        "                break\n",
        "            if len( tokens_a ) > len( tokens_b ) :\n",
        "                tokens_a.pop()\n",
        "            else :\n",
        "                tokens_b.pop()\n",
        "\n",
        "    def get_random_word( vocab_words ) :\n",
        "        i = randint( 0, len( vocab_words ) - 1 )\n",
        "        return vocab_words[ i ]\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_Zih3E3Fqop"
      },
      "source": [
        "class Generator( nn.Module ) :\n",
        "\n",
        "    def __init__( self, config ) :\n",
        "        super().__init__()\n",
        "        self.transformer    = Transformer( config )\n",
        "        self.fc             = nn.Linear( config.dim, config.dim )\n",
        "        self.activ1         = nn.Tanh()\n",
        "        self.linear         = nn.Linear( config.dim, config.dim )\n",
        "        self.activ2         = nn.ReLU()\n",
        "        self.norm           = LayerNorm( config )\n",
        "        self.classifier     = nn.Linear( config.dim, 2 )\n",
        "\n",
        "        # Decoder\n",
        "        embed_weight        = self.transformer\n",
        "                                  .embedding\n",
        "                                  .tok_embed\n",
        "                                  .weight\n",
        "        n_vocab, n_dim      = embed_weight.size()\n",
        "        self.decoder        = nn.Linear( n_dim, n_vocab, bias=False )\n",
        "        self.decoder.weight = embed_weight\n",
        "        self.decoder_bias   = nn.Parameter( torch.zeros( n_vocab) )\n",
        "\n",
        "    def forward( self, input_idx, segment_idx, input_mask, masked_pos ) :\n",
        "        h = self.transformer( input_idx, segment_id, input_mask, masked_pos )\n",
        "        pooled_h = self.activ1( self.fc( h[:,0] ) )\n",
        "        masked_pos = masked_pos[:, :, None].expand( -1, -1, h.size( -1 ) )\n",
        "        h_masked = torch.gather( h, 1, masked_pos )\n",
        "        h_masked = self.norm( self.active2( self.linear( h_masked ) ) )\n",
        "        logits_lm = self.decoder( h_masked ) + self.decoder_bias\n",
        "        logits_clsf = self.classifier( pooled_h )\n",
        "\n",
        "        return logits_lm, logits_clsf"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}